<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aston Zhang</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-27909669-4', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>


<link rel="stylesheet" href="styles.css">
<meta name="twitter:title" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Aston Zhang</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./papers.html">
 <span class="menu-text">Papers</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">



<div class="quarto-about-trestles">
  <div class="about-entity">
    <img src="img/aston.jpg" class="about-image
  rounded " style="width: 200px;">
    <div class="about-links">
  <a href="https://twitter.com/astonzhangAZ" class="about-link">
    <i class="bi bi-twitter"></i>
     <span class="about-link-text">Twitter</span>
  </a>
  <a href="https://github.com/astonzhang" class="about-link">
    <i class="bi bi-github"></i>
     <span class="about-link-text">GitHub</span>
  </a>
</div>
  </div>
  <div class="about-contents"><div id="az-heading">
<p><strong>Aston Zhang</strong> is a research scientist at Meta Generative AI, building large language models. Previously, he studied multimodality for AI and language models at Amazon Web Services AI Research. He received an <a href="https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab" target="_blank">ICLR Outstanding Paper Award</a>, an ACM Ubicomp Distinguished Paper Award, and an ACM SenSys Best Paper Award Nomination. His <a href="https://github.com/d2l-ai" target="_blank">open-source</a> textbook, <a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a>, has been <a href="https://d2l.ai/_images/map.png" target="_blank">adopted worldwide</a>. He obtained his Ph.D.&nbsp;in Computer Science from University of Illinois at Urbana-Champaign.</p>
</div></div>
</div>

<section id="books" class="level2">
<h2 data-anchor-id="books">Books</h2>
<ul>
<li><strong>A. Zhang</strong>, Z. C. Lipton, M. Li, and A. J. Smola<br> <a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a><br> <em>Cambridge University Press</em>, 2023<br>
<ul>
<li>Adopted at 500 universities from 70 countries</li>
<li>Featured in the <a href="https://youtu.be/ue9aumC7AAk?t=6856" target="_blank">AWS re:Invent keynote</a> by Swami, Head of AWS AI, Database, and Analytics<br> <iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&amp;repo=d2l-en&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe></li>
</ul></li>
<li><strong>A. Zhang</strong>, M. Li, Z. C. Lipton, and A. J. Smola<br> <a href="https://zh.d2l.ai" target="_blank">动手学深度学习</a><br> <em>人民邮电出版社</em>, 2nd ed., 2023, 1st ed., 2019<br>
<ul>
<li><a href="https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/sales/jd-2023020304-all-ai-zh-4.png" target="_blank">Best seller</a> in China <br> <iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&amp;repo=d2l-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe></li>
</ul></li>
</ul>
</section>
<section id="papers-all" class="level2">
<h2 data-anchor-id="papers-all">Papers (<a href="./papers.html">All</a>)</h2>
<ul>
<li><p>Z. Zhang, <strong>A. Zhang</strong>, M. Li, H. Zhao, G. Karypis, and A. J. Smola<br> <a href="https://arxiv.org/abs/2302.00923" target="_blank">Multimodal Chain-of-Thought Reasoning in Language Models</a><br> <em>“Imagine reading a book without figures: Multimodal-CoT surpasses humans on ScienceQA.”</em> In <em>arXiv</em>, 2023<br> <a href="https://twitter.com/astonzhangAZ/status/1621545166049005568" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/amazon-science/mm-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a> <a href="img/mm-cot-idea.png" target="_blank">[Idea Inspiration by Homeschooling]</a></p></li>
<li><p>C. Qin, <strong>A. Zhang</strong>, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang<br> <a href="https://arxiv.org/abs/2302.06476" target="_blank">Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</a><br> <em>“Can ChatGPT solve, for example, sequence tagging?”</em> In <em>arXiv</em>, 2023<br></p></li>
<li><p>Z. Zhang and <strong>A. Zhang</strong><br> <a href="https://arxiv.org/abs/2309.11436" target="_blank">You Only Look at Screens: Multimodal Chain-of-Action Agents</a><br> <em>“Perform a task on smart phones? Train an agent using screenshots.”</em> In <em>arXiv</em>, 2023<br> <a href="https://twitter.com/astonzhangAZ/status/1705235421280797111" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/cooelf/Auto-UI" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p>S. Ren, <strong>A. Zhang</strong>, Y. Zhu, S. Zhang, S. Zheng, M. Li, A. J. Smola, X. Sun <br> <a href="https://arxiv.org/abs/2304.04704" target="_blank">Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition</a><br> In <em>Proceedings of the Conference on Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), 2023<br> <a href="https://github.com/amazon-science/prompt-pretraining" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p>Z. Zeng, C. Hawkins, M. Hong, <strong>A. Zhang</strong>, N. Pappas, V. Singh, and S. Zheng<br> <a href="https://arxiv.org/abs/2305.04241" target="_blank">Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</a><br> In <em>Proceedings of the Conference on Neural Information Processing Systems</em> (<strong>NeurIPS</strong>), 2023<br></p></li>
<li><p>J. Chen, <strong>A. Zhang</strong>, X. Shi, M. Li, A. J. Smola, and D. Yang<br> <a href="https://arxiv.org/abs/2301.01821" target="_blank">Parameter-Efficient Fine-Tuning Design Spaces</a><br> In <em>Proceedings of the International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2023<br> <a href="https://twitter.com/astonzhangAZ/status/1611400421255557122" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/amazon-science/peft-design-spaces" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p>Z. Zhang, <strong>A. Zhang</strong>, M. Li, and A. J. Smola<br> <a href="https://arxiv.org/abs/2210.03493" target="_blank">Automatic Chain of Thought Prompting in Large Language Models</a><br> In <em>Proceedings of the International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2023<br> <a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/amazon-research/auto-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p>Z. Liu, Z. Tang, X. Shi, <strong>A. Zhang</strong>, M. Li, A. Shrivastava, and A. Wilson<br> <a href="https://arxiv.org/abs/2212.14453" target="_blank">Learning Multimodal Data Augmentation in Feature Space</a><br> In <em>Proceedings of the International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2023<br></p></li>
<li><p>T. Yang, Y. Zhu, Y. Xie, <strong>A. Zhang</strong>, C. Chen, and M. Li<br> <a href="https://openreview.net/forum?id=CIoSZ_HKHS7" target="_blank">AIM: Adapting Image Models for Efficient Video Understanding</a><br> In <em>Proceedings of the International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2023<br></p></li>
<li><p>H. Wang, <strong>A. Zhang</strong>, Y. Zhu, S. Zheng, M. Li, A. J. Smola, and Z. Wang<br> <a href="https://arxiv.org/pdf/2207.01160.pdf" target="_blank">Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition</a><br> In <em>Proceedings of International Conference on Machine Learning</em> (<strong>ICML, Long Presentation</strong>), 2022<br> <a href="https://twitter.com/astonzhangAZ/status/1549800894840971265" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/amazon-research/long-tailed-ood-detection" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p>H. Wang, <strong>A. Zhang</strong>, S. Zheng, X. Shi, M. Li, and Z. Wang<br> <a href="https://arxiv.org/pdf/2207.01156.pdf" target="_blank">Removing Batch Normalization Boosts Adversarial Training</a><br> In <em>Proceedings of International Conference on Machine Learning</em> (<strong>ICML</strong>), 2022<br> <a href="https://twitter.com/astonzhangAZ/status/1549069378250874880" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a> <a href="https://github.com/amazon-research/normalizer-free-robust-training" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
<li><p><strong>A. Zhang</strong>, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu<br> <a href="https://arxiv.org/abs/2102.08597" target="_blank">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br> In <em>Proceedings of the International Conference on Learning Representations</em> (<strong>ICLR, <font color="red">Outstanding Paper Award</font></strong>), 2021<br> <a href="https://github.com/astonzhang/Parameterization-of-Hypercomplex-Multiplications" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a></p></li>
</ul>
</section>
<section id="tutorials" class="level2">
<h2 data-anchor-id="tutorials">Tutorials</h2>
<ul>
<li><p>with A. J. Smola<br> <strong>Attention in Deep Learning</strong> [<a href="http://alex.smola.org/talks/ICML19-attention.key" target="_blank">Keynote</a>] [<a href="http://alex.smola.org/talks/ICML19-attention.pdf" target="_blank">PDF</a>] [<a href="https://www.youtube.com/watch?v=nS1Lse2B48w" target="_blank">Video</a>]<br> In <em>The 36th International Conference on Machine Learning</em> (<strong>ICML</strong>), 2019</p></li>
<li><p>with H. Lin, X. Shi, L. Lausen, H. He, S. Zha, and A. J. Smola<br> <strong>Dive into Deep Learning for Natural Language Processing</strong> <br> In <em>Conference on Empirical Methods in Natural Language Processing</em> (<strong>EMNLP</strong>), 2019</p></li>
<li><p>with H. Lin, L. Lausen, S. Zha, A. J. Smola, C. Wang, and M. Li<br> <strong>From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond</strong> [<a href="https://github.com/astonzhang/KDD19-tutorial" target="_blank">Website</a>]<br> In <em>The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (<strong>KDD</strong>), 2019</p></li>
<li><p>with H. Zhang, T. He, Z. Zhang, Z. Zhang, H. Lin, and M. Li<br> <strong>Everything You Need to Know to Reproduce SOTA Deep Learning Models from Hands-on Tutorial</strong><br> In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), 2019</p></li>
</ul>
</section>
<section id="services" class="level2">
<h2 data-anchor-id="services">Services</h2>
<ul>
<li>Area Chair
<ul>
<li><em>Annual Meeting of the Association for Computational Linguistics</em> (<strong>ACL</strong>)</li>
<li><em>Conference on Empirical Methods in Natural Language Processing</em> (<strong>EMNLP</strong>)</li>
<li><em>International Conference on Computational Linguistics</em> (<strong>COLING</strong>)</li>
</ul></li>
</ul>
<br> <a href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-size="large" data-show-count="false">Follow <span class="citation" data-cites="astonzhangAZ">@astonzhangAZ</span></a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<br> <a class="twitter-timeline" data-width="350" data-height="200" href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw">Tweets by astonzhangAZ</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>