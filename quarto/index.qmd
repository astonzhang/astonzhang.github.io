---
about:
  id: az-heading
  template: trestles
  image: img/aston.jpg
  image-width: 160px
  image-shape: rounded
  links:
    - icon: bi-envelope
      href: img/email.png

---

:::{#az-heading}

**Aston Zhang** is a member of technical staff at <a href="https://openai.com" target="_blank">OpenAI</a>. 
His work has been recognized with the <a href="https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab" target="_blank">ICLR Outstanding Paper Award</a>,
the ACM Ubicomp Distinguished Paper Award,
and an ACM SenSys Best Paper Award nomination.
His textbook, "<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a>," 
is <a href="https://d2l.ai/_images/map.png" target="_blank">adopted worldwide</a>.
He earned his Ph.D. from the University of Illinois Urbana-Champaign.







:::


## Books

* **A. Zhang**, Z. C. Lipton, M. Li, and A. J. Smola<br>
 	<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a><br>
	*Cambridge University Press*, 2023<br>
	* Adopted at 500 universities from 70 countries
	* <a href="https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/sales/jd-2023020304-all-ai-zh-4.png" target="_blank">Best seller</a> in China 
	<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-en&type=star&count=true" frameborder="0" scrolling="0" width="120" height="30" title="GitHub"></iframe> <iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-zh&type=star&count=true" frameborder="0" scrolling="0" width="120" height="30" title="GitHub"></iframe>



## Papers ([All](papers.qmd))

* M. Zhong\*, **A. Zhang**\*, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten<br>
<a href="https://arxiv.org/abs/2409.19951" target="_blank">Law of the Weakest Link: Cross Capabilities of Large Language Models</a><br>
In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2025<br>
<a href="https://x.com/astonzhangAZ/status/1841131443420008918" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/facebookresearch/llm-cross-capabilities" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>
<a href="https://www.llm-cross-capabilities.org" target="_blank">llm-cross-capabilities.org</a>

* Llama Team, AI@Meta (**Core Contributor**)<br>
<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank">The Llama 3 Herd of Models</a><br>
2024<br>
<a href="https://x.com/astonzhangAZ/status/1815763885380747422" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/meta-llama/llama-models" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>


* Z. Zhang and **A. Zhang**<br>
<a href="https://arxiv.org/abs/2309.11436" target="_blank">You Only Look at Screens: Multimodal Chain-of-Action Agents</a><br>
In *Findings of the Association for Computational Linguistics* (**ACL**), 2024<br>
<a href="https://twitter.com/astonzhangAZ/status/1705235421280797111" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/cooelf/Auto-UI" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zhang, **A. Zhang**, M. Li, H. Zhao, G. Karypis, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2302.00923" target="_blank">Multimodal Chain-of-Thought Reasoning in Language Models</a><br>
	In *Transactions on Machine Learning Research*  (**TMLR**), 2024<br>
	<a href="https://twitter.com/astonzhangAZ/status/1621545166049005568" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/mm-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>
	<a href="img/mm-cot-idea.png" target="_blank">[Idea Inspiration by Homeschooling]</a>

* J. Chen, **A. Zhang**, X. Shi, M. Li, A. J. Smola, and D. Yang<br>
	<a href="https://arxiv.org/abs/2301.01821" target="_blank">Parameter-Efficient Fine-Tuning Design Spaces</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1611400421255557122" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/peft-design-spaces" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* T. Yang, Y. Zhu, Y. Xie, **A. Zhang**, C. Chen, and M. Li<br>
	<a href="https://openreview.net/forum?id=CIoSZ_HKHS7" target="_blank">AIM: Adapting Image Models for Efficient Video Understanding</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>

* Z. Zhang, **A. Zhang**, M. Li, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2210.03493" target="_blank">Automatic Chain of Thought Prompting in Large Language Models</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/auto-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* C. Qin, **A. Zhang**, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang<br>
	<a href="https://arxiv.org/abs/2302.06476" target="_blank">Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</a><br>
	In *Empirical Methods in Natural Language Processing* (**EMNLP**), 2023<br>


* **A. Zhang**, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu<br>
	<a href="https://arxiv.org/abs/2102.08597" target="_blank">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR, <font color="red">Outstanding Paper Award</font>**), 2021<br>
	<a href="https://github.com/astonzhang/Parameterization-of-Hypercomplex-Multiplications" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>




## Tutorials

* with A. J. Smola<br>
	**Attention in Deep Learning** [<a href="http://alex.smola.org/talks/ICML19-attention.key" target="_blank">Keynote</a>] [<a href="http://alex.smola.org/talks/ICML19-attention.pdf" target="_blank">PDF</a>] [<a href="https://www.youtube.com/watch?v=nS1Lse2B48w" target="_blank">Video</a>]<br>
	In *The 36th International Conference on Machine Learning* (**ICML**), 2019


## Services

* Area Chair
	* *Annual Meeting of the Association for Computational Linguistics* (**ACL**)
	* *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**)
	* *International Conference on Computational Linguistics* (**COLING**)
