---
about:
  id: az-heading
  template: trestles
  image: img/aston.jpg
  image-width: 200px
  image-shape: rounded
  links:
    - icon: twitter
      text: Twitter
      href: https://twitter.com/astonzhangAZ
    - icon: github
      text: GitHub
      href: https://github.com/astonzhang
    - icon: bi-envelope
      text: Email
      href: img/email.png
---

:::{#az-heading}

**Aston Zhang** is a senior scientist at Amazon Web Services AI Research.
He studies efficient and robust methods for machine learning and natural language processing, currently focusing on large language models.
He received the <a href="https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab" target="_blank">ICLR Outstanding Paper Award (2021)</a>,
the ACM Ubicomp Distinguished Paper Award (2018), and the ACM SenSys Best Paper Award Nomination (2017).
His <a href="https://github.com/d2l-ai" target="_blank">open-source</a> textbook,
<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a>,
has been <a href="https://d2l.ai/_images/map.png" target="_blank">adopted worldwide</a>.
He obtained his Ph.D. in Computer Science from University of Illinois at Urbana-Champaign.

:::

Hi! I'm happy to connect and discuss.
My team is hiring intern scientists
at our <a href="https://www.google.com/maps/place/2795+Augustine+Dr,+Santa+Clara,+CA+95054/@37.3835062,-121.9778599,17z/data=!3m1!4b1!4m5!3m4!1s0x808fc9f208b94da1:0xd2df9ae053c51af0!8m2!3d37.383502!4d-121.9756659" target="_blank">Santa Clara office</a> or other global offices. Just email me.


## Books

* **A. Zhang**, Z. C. Lipton, M. Li, and A. J. Smola<br>
 	<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a><br>
	*Cambridge University Press*, 2023<br>
	* Adopted at 400 universities from 60 countries
	* Featured in the <a href="https://youtu.be/ue9aumC7AAk?t=6856" target="_blank">AWS re:Invent 2021 keynote</a> by Dr. Swami Sivasubramanian<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-en&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>

* **A. Zhang**, M. Li, Z. C. Lipton, and A. J. Smola<br>
	<a href="https://zh.d2l.ai" target="_blank">动手学深度学习</a><br>
	*人民邮电出版社*, 2nd ed., 2023, 1st ed., 2019<br>
	* <a href="https://github.com/d2l-ai/d2l-zh/raw/v1/img/frontpage/jd-190715-en.png" target="_blank">Best seller</a> of new books in "Computers and Internet" at the largest Chinese online bookstore<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-zh&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>


## Papers ([All](papers.qmd))

* Z. Zhang, **A. Zhang**, M. Li, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2210.03493" target="_blank">Automatic Chain of Thought Prompting in Large Language Models</a><br>
	*"Let's think not just step by step, but also one by one."*
	In *arXiv*, 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/auto-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>


* J. Chen, **A. Zhang**, X. Shi, M. Li, A. J. Smola, and D. Yang<br>
	<a href="https://arxiv.org/abs/2301.01821" target="_blank">Parameter-Efficient Fine-Tuning Design Spaces</a><br>
	*"Don't assign the same parameter-efficient fine-tuning strategy to different layers."*
	In *arXiv*, 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1611400421255557122" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/peft-design-spaces" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>


* M. S. Bari, **A. Zhang**, S. Zheng, X. Shi, Y. Zhu, S. Joty, and M. Li<br>
	<a href="https://arxiv.org/abs/2212.10929" target="_blank">
	SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning</a><br>
	*"If your prompt tuning can't converge easily, make it semi-parametric."*
	In *arXiv*, 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1605959819894349824" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>

* H. Wang, **A. Zhang**, Y. Zhu, S. Zheng, M. Li, A. J. Smola, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01160.pdf" target="_blank">Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML, Long Presentation**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549800894840971265" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/long-tailed-ood-detection" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* H. Wang, **A. Zhang**, S. Zheng, X. Shi, M. Li, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01156.pdf" target="_blank">Removing Batch Normalization Boosts Adversarial Training</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549069378250874880" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/normalizer-free-robust-training" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* E. Grassucci, **A. Zhang**, and D. Comminiello<br>
	<a href="https://arxiv.org/abs/2110.04176" target="_blank">PHNNs: Lightweight Neural Networks via Parameterized Hypercomplex Convolutions</a><br>
	In *IEEE Transactions on Neural Networks and Learning Systems* (**TNNLS**), 2022<br>
	<a href="https://github.com/eleGAN23/HyperNets" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* **A. Zhang**, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu<br>
	<a href="https://arxiv.org/abs/2102.08597" target="_blank">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR, <font color="red">Outstanding Paper Award</font>**), 2021<br>
	<a href="https://github.com/astonzhang/Parameterization-of-Hypercomplex-Multiplications" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>




## Tutorials

* with A. J. Smola<br>
	**Attention in Deep Learning** [<a href="http://alex.smola.org/talks/ICML19-attention.key" target="_blank">Keynote</a>] [<a href="http://alex.smola.org/talks/ICML19-attention.pdf" target="_blank">PDF</a>] [<a href="https://www.youtube.com/watch?v=nS1Lse2B48w" target="_blank">Video</a>]<br>
	In *The 36th International Conference on Machine Learning* (**ICML**), 2019

* with H. Lin, X. Shi, L. Lausen, H. He, S. Zha, and A. J. Smola<br>
	**Dive into Deep Learning for Natural Language Processing** <br>
	In *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**), 2019

* with H. Lin, L. Lausen, S. Zha, A. J. Smola, C. Wang, and M. Li<br>
	**From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond** [<a href="https://github.com/astonzhang/KDD19-tutorial" target="_blank">Website</a>]<br>
	In *The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (**KDD**), 2019

* with H. Zhang, T. He, Z. Zhang, Z. Zhang, H. Lin, and M. Li<br>
	**Everything You Need to Know to Reproduce SOTA Deep Learning Models from Hands-on Tutorial**<br>
	In *International Conference on Computer Vision* (**ICCV**), 2019



## Services

* Area Chair
	* *Annual Meeting of the Association for Computational Linguistics* (**ACL**)
	* *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**)

* Senior Program Committee
	* *AAAI Conference on Artificial Intelligence* (**AAAI**)

* Journal Editorial Board
	* *Frontiers in Big Data*


## Ph.D. Interns</a>

* <a href="https://sites.cc.gatech.edu/~jchen896/" target="_blank">Jiaao Chen</a> (2022)
* <a href="https://bcmi.sjtu.edu.cn/home/zhangzs/" target="_blank">Zhuosheng Zhang</a> (2022)
* <a href="https://sbmaruf.github.io/" target="_blank">M Saiful Bari</a> (2022)
* <a href="https://renshuhuai-andy.github.io/" target="_blank">Shuhuai Ren</a> (2022)
* <a href="https://htwang14.github.io/" target="_blank">Haotao Wang</a> (2021, 2022)
* <a href="https://shuaizhang.tech/" target="_blank">Shuai Zhang</a> (2019&ndash;2020)

<br>
<a href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-size="large" data-show-count="false">Follow @astonzhangAZ</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<br>
<a class="twitter-timeline" data-width="350" data-height="200" href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw">Tweets by astonzhangAZ</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
