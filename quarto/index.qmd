---
about:
  id: az-heading
  template: trestles
  image: img/aston.jpg
  image-width: 200px
  image-shape: rounded
  links:
    - icon: twitter
      href: https://twitter.com/astonzhangAZ
    - icon: bi-envelope
      href: img/email.png

---

:::{#az-heading}

**Aston Zhang** is an AI researcher at OpenAI. He was the long context lead for Llama at Meta Superintelligence Labs, where he contributed the 10M-token <a href="https://video.twimg.com/amplify_video/1912183288329908225/vid/avc1/2048x1080/GmFJ_4i-QUBuqytV.mp4" target="_blank">multimodal context</a> with the <a href="https://x.com/astonzhangAZ/status/1908595612372885832" target="_blank">iRoPE</a> architecture. His work has been recognized with the <a href="https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab" target="_blank">ICLR Outstanding Paper Award</a>,
the ACM Ubicomp Distinguished Paper Award,
and an ACM SenSys Best Paper Award nomination.
His textbook, "<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a>," 
is <a href="https://d2l.ai/_images/map.png" target="_blank">adopted worldwide</a>.
He earned his Ph.D. from UIUC.







:::

## News

* Excited to share <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank">Llama 4</a>’s 10M-token <a href="https://video.twimg.com/amplify_video/1912183288329908225/vid/avc1/2048x1080/GmFJ_4i-QUBuqytV.mp4" target="_blank">multimodal context</a> (20+ hours of video) with the <a href="https://x.com/astonzhangAZ/status/1908595612372885832" target="_blank">iRoPE</a> architecture!
* <a href="https://ai.meta.com/blog/meta-llama-3-1/" target="_blank">Llama 3.1 405B</a> is now openly available. <a href="https://x.com/astonzhangAZ/status/1815763885380747422" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
* Meet <a href="https://ai.meta.com/blog/meta-llama-3/" target="_blank">Llama 3</a>, our state-of-the-art open source large language model. Check out <a href="https://x.com/AIatMeta/status/1806757387677897206" target="_blank">my developer podcast</a>. <a href="https://x.com/astonzhangAZ/status/1780990210576441844" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>





## Books

* **A. Zhang**, Z. C. Lipton, M. Li, and A. J. Smola<br>
 	<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a><br>
	*Cambridge University Press*, 2023<br>
	* Adopted at 500 universities from 70 countries
	* Featured in the <a href="https://youtu.be/ue9aumC7AAk?t=6856" target="_blank">AWS re:Invent keynote</a> by Swami, Head of AWS AI, Database, and Analytics<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-en&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>

* **A. Zhang**, M. Li, Z. C. Lipton, and A. J. Smola<br>
	<a href="https://zh.d2l.ai" target="_blank">动手学深度学习</a><br>
	*人民邮电出版社*, 2nd ed., 2023, 1st ed., 2019<br>
	* <a href="https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/sales/jd-2023020304-all-ai-zh-4.png" target="_blank">Best seller</a> in China 
	<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-zh&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>


## Papers ([All](papers.qmd))

* M. Zhong\*, **A. Zhang**\*, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten (\*equal contribution)<br>
<a href="https://arxiv.org/abs/2409.19951" target="_blank">Law of the Weakest Link: Cross Capabilities of Large Language Models</a><br>
In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2025<br>
<a href="https://x.com/astonzhangAZ/status/1841131443420008918" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/facebookresearch/llm-cross-capabilities" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>
<a href="https://www.llm-cross-capabilities.org" target="_blank">llm-cross-capabilities.org</a>


* J. Kim, A. Goyal, **A. Zhang**, B. Xiong, R. Hou, M. Kambadur, D. Mahajan, H. Hajishirzi, and L. Tan<br>
<a href="https://arxiv.org/abs/2412.15282">A Systematic Examination of Preference Learning through the Lens of Instruction-Following</a><br>
In *Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics* (**NAACL**), 2025<br>


* Y. Yu, Z. Chen, **A. Zhang**, L. Tan, C. Zhu, R. Y. Pang, Y. Qian, X. Wang, S. Gururangan, C. Zhang, M. Kambadur, D. Mahajan, and R. Hou<br>
<a href="https://arxiv.org/abs/2411.16646">Self-Generated Critiques Boost Reward Modeling for Language Models</a><br>
In *Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics* (**NAACL**), 2025<br>

* Llama Team, AI@Meta (**Core Contributor**)<br>
<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" target="_blank">The Llama 3 Herd of Models</a><br>
2024<br>
<a href="https://x.com/astonzhangAZ/status/1815763885380747422" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/meta-llama/llama-models" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>


* Z. Zhang and **A. Zhang**<br>
<a href="https://arxiv.org/abs/2309.11436" target="_blank">You Only Look at Screens: Multimodal Chain-of-Action Agents</a><br>
In *Findings of the Association for Computational Linguistics* (**ACL**), 2024<br>
<a href="https://twitter.com/astonzhangAZ/status/1705235421280797111" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/cooelf/Auto-UI" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zhang, **A. Zhang**, M. Li, H. Zhao, G. Karypis, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2302.00923" target="_blank">Multimodal Chain-of-Thought Reasoning in Language Models</a><br>
	In *Transactions on Machine Learning Research*  (**TMLR**), 2024<br>
	<a href="https://twitter.com/astonzhangAZ/status/1621545166049005568" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/mm-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>
	<a href="img/mm-cot-idea.png" target="_blank">[Idea Inspiration by Homeschooling]</a>

* S. Ren, **A. Zhang**, Y. Zhu, S. Zhang, S. Zheng, M. Li, A. J. Smola, X. Sun
<br>
<a href="https://arxiv.org/abs/2304.04704" target="_blank">Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition</a><br>
In *Proceedings of the Conference on Neural Information Processing Systems* (**NeurIPS**), 2023<br>
<a href="https://github.com/amazon-science/prompt-pretraining" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zeng, C. Hawkins, M. Hong, **A. Zhang**, N. Pappas, V. Singh, and S. Zheng<br>
<a href="https://arxiv.org/abs/2305.04241" target="_blank">Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</a><br>
In *Proceedings of the Conference on Neural Information Processing Systems* (**NeurIPS**), 2023<br>

* J. Chen, **A. Zhang**, X. Shi, M. Li, A. J. Smola, and D. Yang<br>
	<a href="https://arxiv.org/abs/2301.01821" target="_blank">Parameter-Efficient Fine-Tuning Design Spaces</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1611400421255557122" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/peft-design-spaces" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zhang, **A. Zhang**, M. Li, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2210.03493" target="_blank">Automatic Chain of Thought Prompting in Large Language Models</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/auto-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Liu, Z. Tang, X. Shi, **A. Zhang**, M. Li, A. Shrivastava, and A. Wilson<br>
	<a href="https://arxiv.org/abs/2212.14453" target="_blank">Learning Multimodal Data Augmentation in Feature Space</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>

* T. Yang, Y. Zhu, Y. Xie, **A. Zhang**, C. Chen, and M. Li<br>
	<a href="https://openreview.net/forum?id=CIoSZ_HKHS7" target="_blank">AIM: Adapting Image Models for Efficient Video Understanding</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>


* H. Wang, **A. Zhang**, Y. Zhu, S. Zheng, M. Li, A. J. Smola, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01160.pdf" target="_blank">Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML, Long Presentation**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549800894840971265" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/long-tailed-ood-detection" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* H. Wang, **A. Zhang**, S. Zheng, X. Shi, M. Li, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01156.pdf" target="_blank">Removing Batch Normalization Boosts Adversarial Training</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549069378250874880" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/normalizer-free-robust-training" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* **A. Zhang**, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu<br>
	<a href="https://arxiv.org/abs/2102.08597" target="_blank">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR, <font color="red">Outstanding Paper Award</font>**), 2021<br>
	<a href="https://github.com/astonzhang/Parameterization-of-Hypercomplex-Multiplications" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>




## Tutorials

* with A. J. Smola<br>
	**Attention in Deep Learning** [<a href="http://alex.smola.org/talks/ICML19-attention.key" target="_blank">Keynote</a>] [<a href="http://alex.smola.org/talks/ICML19-attention.pdf" target="_blank">PDF</a>] [<a href="https://www.youtube.com/watch?v=nS1Lse2B48w" target="_blank">Video</a>]<br>
	In *The 36th International Conference on Machine Learning* (**ICML**), 2019


## Services

* Area Chair
	* *Annual Meeting of the Association for Computational Linguistics* (**ACL**)
	* *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**)
	* *International Conference on Computational Linguistics* (**COLING**)


<br>
<a href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-size="large" data-show-count="false">Follow @astonzhangAZ</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<br>
<a class="twitter-timeline" data-width="350" data-height="200" href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw">Tweets by astonzhangAZ</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
