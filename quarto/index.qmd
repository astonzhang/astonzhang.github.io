---
about:
  id: az-heading
  template: trestles
  image: img/aston.jpg
  image-width: 200px
  image-shape: rounded
  links:
    - icon: twitter
      href: https://twitter.com/astonzhangAZ
    - icon: bi-envelope
      href: img/email.png
    - icon: github
      href: https://github.com/astonzhang
    - icon: google
      href: https://scholar.google.com/citations?user=WXOaxKMAAAAJ

---

:::{#az-heading}

**Aston Zhang** is a research scientist at Meta Generative AI, building large language models (Llama).
Prior to this, he was a scientist/manager at Amazon Web Services AI Research,
studying language and multimodal models.
He received an <a href="https://iclr-conf.medium.com/announcing-iclr-2021-outstanding-paper-awards-9ae0514734ab" target="_blank">ICLR Outstanding Paper Award</a>,
an ACM Ubicomp Distinguished Paper Award, and an ACM SenSys Best Paper Award Nomination.
His <a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a> textbook
is <a href="https://d2l.ai/_images/map.png" target="_blank">adopted worldwide</a>.
He obtained a Ph.D. in Computer Science from University of Illinois Urbana-Champaign.

:::

*If you are interested in <font color="#ff0000">research internship</font> on large language models at the Llama team in 2024, feel free to <a href="img/email.png" target="_blank">email me</a>.*


## Books

* **A. Zhang**, Z. C. Lipton, M. Li, and A. J. Smola<br>
 	<a href="https://d2l.ai" target="_blank">Dive into Deep Learning</a><br>
	*Cambridge University Press*, 2023<br>
	* Adopted at 500 universities from 70 countries (on <a href="https://www.amazon.com/Dive-into-Learning-Aston-Zhang/dp/1009389432" target="_blank">Amazon</a>)
	* Featured in the <a href="https://youtu.be/ue9aumC7AAk?t=6856" target="_blank">AWS re:Invent keynote</a> by Swami, Head of AWS AI, Database, and Analytics<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-en&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>

* **A. Zhang**, M. Li, Z. C. Lipton, and A. J. Smola<br>
	<a href="https://zh.d2l.ai" target="_blank">动手学深度学习</a><br>
	*人民邮电出版社*, 2nd ed., 2023, 1st ed., 2019<br>
	* <a href="https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/sales/jd-2023020304-all-ai-zh-4.png" target="_blank">Best seller</a> in China 
	<br>
	<iframe src="https://ghbtns.com/github-btn.html?user=d2l-ai&repo=d2l-zh&type=star&count=true" frameborder="0" scrolling="0" width="150" height="30" title="GitHub"></iframe>


## Papers ([All](papers.qmd))

* Z. Zhang and **A. Zhang**<br>
<a href="https://arxiv.org/abs/2309.11436" target="_blank">You Only Look at Screens: Multimodal Chain-of-Action Agents</a><br>
*"Perform a task on smart phones? Train an agent using screenshots."*
In *arXiv*, 2023<br>
<a href="https://twitter.com/astonzhangAZ/status/1705235421280797111" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
<a href="https://github.com/cooelf/Auto-UI" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zhang, **A. Zhang**, M. Li, H. Zhao, G. Karypis, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2302.00923" target="_blank">Multimodal Chain-of-Thought Reasoning in Language Models</a><br>
	*"Imagine reading a book without figures: Multimodal-CoT surpasses humans on ScienceQA."*
	In *arXiv*, 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1621545166049005568" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/mm-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>
	<a href="img/mm-cot-idea.png" target="_blank">[Idea Inspiration by Homeschooling]</a>

* S. Ren, **A. Zhang**, Y. Zhu, S. Zhang, S. Zheng, M. Li, A. J. Smola, X. Sun
<br>
<a href="https://arxiv.org/abs/2304.04704" target="_blank">Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition</a><br>
In *Proceedings of the Conference on Neural Information Processing Systems* (**NeurIPS**), 2023<br>
<a href="https://github.com/amazon-science/prompt-pretraining" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zeng, C. Hawkins, M. Hong, **A. Zhang**, N. Pappas, V. Singh, and S. Zheng<br>
<a href="https://arxiv.org/abs/2305.04241" target="_blank">Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens</a><br>
In *Proceedings of the Conference on Neural Information Processing Systems* (**NeurIPS**), 2023<br>

* J. Chen, **A. Zhang**, X. Shi, M. Li, A. J. Smola, and D. Yang<br>
	<a href="https://arxiv.org/abs/2301.01821" target="_blank">Parameter-Efficient Fine-Tuning Design Spaces</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1611400421255557122" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-science/peft-design-spaces" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Zhang, **A. Zhang**, M. Li, and A. J. Smola<br>
	<a href="https://arxiv.org/abs/2210.03493" target="_blank">Automatic Chain of Thought Prompting in Large Language Models</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>
	<a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/auto-cot" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* Z. Liu, Z. Tang, X. Shi, **A. Zhang**, M. Li, A. Shrivastava, and A. Wilson<br>
	<a href="https://arxiv.org/abs/2212.14453" target="_blank">Learning Multimodal Data Augmentation in Feature Space</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>

* T. Yang, Y. Zhu, Y. Xie, **A. Zhang**, C. Chen, and M. Li<br>
	<a href="https://openreview.net/forum?id=CIoSZ_HKHS7" target="_blank">AIM: Adapting Image Models for Efficient Video Understanding</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR**), 2023<br>

* C. Qin, **A. Zhang**, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang<br>
<a href="https://arxiv.org/abs/2302.06476" target="_blank">Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</a><br>
In *Empirical Methods in Natural Language Processing* (**EMNLP**), 2023<br>

* J. Chen, **A. Zhang**, D. Yang, M. Li, and A. J. Smola<br>
<a href="https://arxiv.org/abs/2304.04746" target="_blank">A Cheaper and Better Diffusion Language Model with Soft-Masked Noise</a><br>
In *Empirical Methods in Natural Language Processing* (**EMNLP**), 2023<br>
<a href="https://github.com/amazon-science/masked-diffusion-lm" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* H. Wang, **A. Zhang**, Y. Zhu, S. Zheng, M. Li, A. J. Smola, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01160.pdf" target="_blank">Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML, Long Presentation**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549800894840971265" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/long-tailed-ood-detection" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* H. Wang, **A. Zhang**, S. Zheng, X. Shi, M. Li, and Z. Wang<br>
	<a href="https://arxiv.org/pdf/2207.01156.pdf" target="_blank">Removing Batch Normalization Boosts Adversarial Training</a><br>
	In *Proceedings of International Conference on Machine Learning* (**ICML**), 2022<br>
	<a href="https://twitter.com/astonzhangAZ/status/1549069378250874880" title="Tweet" target="_blank"><img src="./img/logo-twitter.svg" class="paper-icon"></a>
	<a href="https://github.com/amazon-research/normalizer-free-robust-training" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>

* **A. Zhang**, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu<br>
	<a href="https://arxiv.org/abs/2102.08597" target="_blank">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br>
	In *Proceedings of the International Conference on Learning Representations* (**ICLR, <font color="red">Outstanding Paper Award</font>**), 2021<br>
	<a href="https://github.com/astonzhang/Parameterization-of-Hypercomplex-Multiplications" title="Code" target="_blank"><img src="./img/logo-github.svg" class="paper-icon"></a>




## Tutorials

* with A. J. Smola<br>
	**Attention in Deep Learning** [<a href="http://alex.smola.org/talks/ICML19-attention.key" target="_blank">Keynote</a>] [<a href="http://alex.smola.org/talks/ICML19-attention.pdf" target="_blank">PDF</a>] [<a href="https://www.youtube.com/watch?v=nS1Lse2B48w" target="_blank">Video</a>]<br>
	In *The 36th International Conference on Machine Learning* (**ICML**), 2019

* with H. Lin, X. Shi, L. Lausen, H. He, S. Zha, and A. J. Smola<br>
	**Dive into Deep Learning for Natural Language Processing** <br>
	In *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**), 2019

* with H. Lin, L. Lausen, S. Zha, A. J. Smola, C. Wang, and M. Li<br>
	**From Shallow to Deep Language Representations: Pre-training, Fine-tuning, and Beyond** [<a href="https://github.com/astonzhang/KDD19-tutorial" target="_blank">Website</a>]<br>
	In *The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (**KDD**), 2019

* with H. Zhang, T. He, Z. Zhang, Z. Zhang, H. Lin, and M. Li<br>
	**Everything You Need to Know to Reproduce SOTA Deep Learning Models from Hands-on Tutorial**<br>
	In *International Conference on Computer Vision* (**ICCV**), 2019


## Services

* Area Chair
	* *Annual Meeting of the Association for Computational Linguistics* (**ACL**)
	* *Conference on Empirical Methods in Natural Language Processing* (**EMNLP**)
	* *International Conference on Computational Linguistics* (**COLING**)


<br>
<a href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-size="large" data-show-count="false">Follow @astonzhangAZ</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<br>
<a class="twitter-timeline" data-width="350" data-height="200" href="https://twitter.com/astonzhangAZ?ref_src=twsrc%5Etfw">Tweets by astonzhangAZ</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
